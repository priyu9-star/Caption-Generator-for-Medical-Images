{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IUc-xYQao4a"
   },
   "outputs": [],
   "source": [
    "import os   # handling the files\n",
    "import pickle # storing numpy features\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm # how much data is process till now\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16 , preprocess_input # extract features from image data.\n",
    "from tensorflow.keras.preprocessing.image import load_img , img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Input , Dense , LSTM , Embedding , Dropout , add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKTCMsYpRKS8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67929,
     "status": "ok",
     "timestamp": 1742360230867,
     "user": {
      "displayName": "0901CD221045 PRIYANSHI JAYANT",
      "userId": "15253078731176606939"
     },
     "user_tz": -330
    },
    "id": "BYHCZf1KassT",
    "outputId": "310d6e09-9157-48f6-8fb7-bd5169e87c5c"
   },
   "outputs": [],
   "source": [
    "! kaggle datasets download -d virajbagal/roco-dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123554,
     "status": "ok",
     "timestamp": 1742360354404,
     "user": {
      "displayName": "0901CD221045 PRIYANSHI JAYANT",
      "userId": "15253078731176606939"
     },
     "user_tz": -330
    },
    "id": "NdUguf08azsM",
    "outputId": "bd848e2b-fcef-4bbb-f802-f5bd15d547d7"
   },
   "outputs": [],
   "source": [
    "!unzip roco-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CICrNSu2a-SK"
   },
   "outputs": [],
   "source": [
    "BASE_DIR = '/content/all_data/train/radiology'\n",
    "WORKING_DIR = '/content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1742360354792,
     "user": {
      "displayName": "0901CD221045 PRIYANSHI JAYANT",
      "userId": "15253078731176606939"
     },
     "user_tz": -330
    },
    "id": "8dUhK-iNbd65",
    "outputId": "0b99a11f-5130-4598-a42e-cff0fc254f8d"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Path to the CSV file\n",
    "input_csv_path = '/content/all_data/train/radiology/traindata.csv'\n",
    "# Path to the output text file\n",
    "output_txt_path = '/content/all_data/train/radiology/output.txt'\n",
    "\n",
    "# Open the CSV file and the output text file\n",
    "with open(input_csv_path, 'r') as csv_file, open(output_txt_path, 'w') as txt_file:\n",
    "    csv_reader = csv.DictReader(csv_file)  # Read the CSV file as a dictionary\n",
    "\n",
    "    for row in csv_reader:\n",
    "        name = row['name']  # Get the 'name' column\n",
    "        caption = row['caption']  # Get the 'caption' column\n",
    "\n",
    "        # Write to the text file in the format: name,caption\n",
    "        txt_file.write(f\"{name},{caption}\")\n",
    "\n",
    "print(f\"Name and caption have been written to {output_txt_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 885
    },
    "executionInfo": {
     "elapsed": 13627,
     "status": "ok",
     "timestamp": 1742360368416,
     "user": {
      "displayName": "0901CD221045 PRIYANSHI JAYANT",
      "userId": "15253078731176606939"
     },
     "user_tz": -330
    },
    "id": "CgQYznm1bq5S",
    "outputId": "7f58d217-1590-4e32-be11-9f89f96db397"
   },
   "outputs": [],
   "source": [
    "# Load vgg16 Model\n",
    "model = VGG16()\n",
    "\n",
    "# restructure model\n",
    "model = Model(inputs = model.inputs , outputs = model.layers[-2].output)\n",
    "\n",
    "# Summerize\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "id": "G4mDeNnYbuYe",
    "outputId": "30ac17b4-6275-472b-f01f-f7ec619f717e"
   },
   "outputs": [],
   "source": [
    "# extract features from image\n",
    "features = {}\n",
    "directory = os.path.join(BASE_DIR, 'images')\n",
    "\n",
    "count=0\n",
    "for img_name in tqdm(os.listdir(directory)):\n",
    "    if count==50:\n",
    "      break\n",
    "    count+=1\n",
    "    # load the image from file\n",
    "    img_path = directory + '/' + img_name\n",
    "    image = load_img(img_path, target_size=(224, 224))\n",
    "    # convert image pixels to numpy array\n",
    "    image = img_to_array(image)\n",
    "    # reshape data for model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # preprocess image for vgg\n",
    "    image = preprocess_input(image)\n",
    "    # extract features\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    # get image ID\n",
    "    image_id = img_name.split('.')[0]\n",
    "    # store feature\n",
    "    features[image_id] = feature\n",
    "    print(feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "8RDV0ax8b27I"
   },
   "outputs": [],
   "source": [
    "# store features in pickle\n",
    "pickle.dump(features, open(os.path.join(WORKING_DIR, 'features.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "rSl6JixUcBt3"
   },
   "outputs": [],
   "source": [
    "# load features from pickle\n",
    "with open(os.path.join(WORKING_DIR, 'features.pkl'), 'rb') as f:\n",
    "    features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "7M6G-Kv3cCaB"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(BASE_DIR, 'output.txt'), 'r') as f:\n",
    "    next(f)\n",
    "    captions_doc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "wXAHOwZscEO1",
    "outputId": "02316881-13fc-4ea4-b73a-afb614da7134"
   },
   "outputs": [],
   "source": [
    "# create mapping of image to captions\n",
    "mapping = {}\n",
    "# process lines\n",
    "for line in tqdm(captions_doc.split('\\n')):\n",
    "    # split the line by comma(,)\n",
    "    tokens = line.split(',')\n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "\n",
    "    image_id, caption = tokens[0], tokens[1:]\n",
    "    # remove extension from image ID\n",
    "    image_id = image_id.split('.')[0]\n",
    "    if image_id not in features.keys():\n",
    "      continue\n",
    "    # convert caption list to string\n",
    "    caption = \" \".join(caption)\n",
    "    # create list if needed\n",
    "    if image_id not in mapping:\n",
    "        mapping[image_id] = []\n",
    "    # store the caption\n",
    "    mapping[image_id].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "KYyZ-4dRcH99",
    "outputId": "f64b54b6-367e-42d0-b1e6-3477b90e41da"
   },
   "outputs": [],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "8W1iZDfNcL7m"
   },
   "outputs": [],
   "source": [
    "def clean(mapping):\n",
    "    for key, captions in mapping.items():\n",
    "        for i in range(len(captions)):\n",
    "            # take one caption at a time\n",
    "            caption = captions[i]\n",
    "            # preprocessing steps\n",
    "            # convert to lowercase\n",
    "            caption = caption.lower()\n",
    "            # delete digits, special chars, etc.,\n",
    "            caption = caption.replace('[^A-Za-z]', '')\n",
    "            # delete additional spaces\n",
    "            caption = caption.replace('\\s+', ' ')\n",
    "            # add start and end tags to the caption\n",
    "            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' endseq'\n",
    "            captions[i] = caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "LwO1e1r2cRe4",
    "outputId": "71443623-d64b-41cd-bda8-e72dd0774b74"
   },
   "outputs": [],
   "source": [
    "# before preprocess of text\n",
    "mapping['PMC3639690_CRIM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "7oi4z6_RcT5T"
   },
   "outputs": [],
   "source": [
    "# preprocess the text\n",
    "clean(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "bu3-_PIncfmU"
   },
   "outputs": [],
   "source": [
    "# before preprocess of text\n",
    "mapping['PMC3639690_CRIM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "W9Eo7uvmchlh"
   },
   "outputs": [],
   "source": [
    "all_captions = []\n",
    "for key in mapping:\n",
    "    for caption in mapping[key]:\n",
    "        all_captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ZbCPn2rFcsZ0"
   },
   "outputs": [],
   "source": [
    "len(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Nvj3N4bDctw1"
   },
   "outputs": [],
   "source": [
    "all_captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "-4S68IMjcv7O"
   },
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "i3e88g4TcyPM"
   },
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "TQZJCOzic0Ev"
   },
   "outputs": [],
   "source": [
    "# get maximum length of the caption available\n",
    "max_length = max(len(caption.split()) for caption in all_captions)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "8JP1T6Kgc1y_"
   },
   "outputs": [],
   "source": [
    "image_ids = list(mapping.keys())\n",
    "split = int(len(image_ids) * 0.90)\n",
    "train = image_ids[:split]\n",
    "test = image_ids[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "CxYP5Ukic7gX"
   },
   "outputs": [],
   "source": [
    "# encoder model\n",
    "# image feature layers\n",
    "\n",
    "inputs1 = Input(shape=(4096,))\n",
    "fe1 = Dropout(0.4)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "\n",
    "# sequence feature layers\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.4)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "# decoder model\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# plot the model\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "bznLFe5Ac3wV"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_keys = list(mapping.keys())  # Keys for all images in the dataset\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "train_keys, val_keys = train_test_split(dataset_keys, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "_F2oiiEsc_pT"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
    "    # Initialize storage\n",
    "    X1, X2, y = [], [], []\n",
    "    n = 0\n",
    "\n",
    "    while True:  # Infinite generator\n",
    "        for key in data_keys:\n",
    "            captions = mapping[key]\n",
    "            for caption in captions:\n",
    "                # Encode the sequence\n",
    "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                for i in range(1, len(seq)):\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "                    # Collect inputs and outputs\n",
    "                    X1.append(features[key][0])  # Features\n",
    "                    X2.append(in_seq)  # Input sequence\n",
    "                    y.append(out_seq)  # Output sequence\n",
    "\n",
    "            # Yield batch when size matches batch_size\n",
    "            n += 1\n",
    "            if n == batch_size:\n",
    "                if len(X1) == 0:  # Skip empty batches\n",
    "                    continue\n",
    "                # Convert to TensorFlow tensors\n",
    "                X1_tensor = tf.convert_to_tensor(np.array(X1), dtype=tf.float32)\n",
    "                X2_tensor = tf.convert_to_tensor(np.array(X2), dtype=tf.float32)\n",
    "                y_tensor = tf.convert_to_tensor(np.array(y), dtype=tf.float32)\n",
    "\n",
    "                yield (X1_tensor, X2_tensor), y_tensor\n",
    "\n",
    "                # Reset storage\n",
    "                X1, X2, y = [], [], []\n",
    "                n = 0\n",
    "\n",
    "\n",
    "output_signature = (\n",
    "    (\n",
    "        tf.TensorSpec(shape=(None, 4096), dtype=tf.float32),  # Features\n",
    "        tf.TensorSpec(shape=(None, max_length), dtype=tf.float32),  # Input sequences\n",
    "    ),\n",
    "    tf.TensorSpec(shape=(None, vocab_size), dtype=tf.float32),  # Output sequences\n",
    ")\n",
    "batch_size=32\n",
    "def safe_data_generator():\n",
    "    try:\n",
    "        yield from data_generator(train_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generator: {e}\")\n",
    "# Create a dataset from the generator\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    safe_data_generator,\n",
    "    output_signature=(\n",
    "        (\n",
    "            tf.TensorSpec(shape=(None, 4096), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, max_length), dtype=tf.float32),\n",
    "        ),\n",
    "        tf.TensorSpec(shape=(None, vocab_size), dtype=tf.float32),\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "steps_per_epoch = len(train_keys) // 32\n",
    "model.fit(train_dataset, epochs=200, steps_per_epoch=steps_per_epoch, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "j0vFPcRNdE-8"
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save(WORKING_DIR+'/best_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Lfj88i2JlJn5"
   },
   "outputs": [],
   "source": [
    "def idx_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "BgS2RdXilLnA"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "def load_trained_model(model_path):\n",
    "    \"\"\"Load a pre-trained model from an .h5 file.\"\"\"\n",
    "    model = load_model(model_path,safe_mode=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "sldSpBbIlNn2"
   },
   "outputs": [],
   "source": [
    "# generate caption for an image\n",
    "def predict_caption(model, image, tokenizer, max_length):\n",
    "    model=keras.models.load_model(\"/content/best_model.keras\")\n",
    "    # add start tag for generation process\n",
    "    # add start tag for generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the max length of sequence\n",
    "    for i in range(max_length):\n",
    "        # encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad the sequence\n",
    "        sequence = pad_sequences([sequence], max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([image, sequence], verbose=0)\n",
    "        # get index with high probability\n",
    "        yhat = np.argmax(yhat)\n",
    "        # convert index to word\n",
    "        word = idx_to_word(yhat, tokenizer)\n",
    "        # stop if word not found\n",
    "        if word is None:\n",
    "            break\n",
    "        # append word as input for generating next word\n",
    "        in_text += \" \" + word\n",
    "        # stop if we reach end tag\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "DemQ1yAolUJH"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "# validate with test data\n",
    "actual, predicted = list(), list()\n",
    "\n",
    "for key in tqdm(train_keys):\n",
    "    # get actual caption\n",
    "    captions = mapping[key]\n",
    "    # predict the caption for image\n",
    "    y_pred = predict_caption(model, features[key], tokenizer, max_length)\n",
    "    # split into words\n",
    "    actual_captions = [caption.split() for caption in captions]\n",
    "    y_pred = y_pred.split()\n",
    "    # append to the list\n",
    "    actual.append(actual_captions)\n",
    "    predicted.append(y_pred)\n",
    "# calcuate BLEU score\n",
    "print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwP5IQJ6mPla"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ZKgNR7BclXJu"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "def generate_caption(image_name):\n",
    "    # load the image\n",
    "    # image_name = \"1001773457_577c3a7d70.jpg\"\n",
    "    image_id = image_name.split('.')[0]\n",
    "    img_path = os.path.join(BASE_DIR, \"images\", image_name)\n",
    "    image = Image.open(img_path)\n",
    "    captions = mapping[image_id]\n",
    "    print('---------------------Actual---------------------')\n",
    "    for caption in captions:\n",
    "        print(caption)\n",
    "    # predict the caption\n",
    "    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n",
    "    print('--------------------Predicted--------------------')\n",
    "    print(y_pred)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "y0DTC0Fsl6fq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory path\n",
    "directory_path = '/content/all_data/train/radiology/images/'\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(directory_path):\n",
    "    # List all files and directories\n",
    "    all_files = os.listdir(directory_path)\n",
    "\n",
    "    # Print the first 5 items\n",
    "    print(\"First 5 files or directories:\")\n",
    "    for item in all_files[:5]:  # Get the first 5\n",
    "        print(item)\n",
    "else:\n",
    "    print(f\"Directory not found: {directory_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Oiz1qud4mA5E"
   },
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "sMDt_iRwmuSS"
   },
   "outputs": [],
   "source": [
    "generate_caption(\"PMC3353704_DRJ-9-233-g004.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "2QzIJ6SEnqmN"
   },
   "outputs": [],
   "source": [
    "generate_caption(\"PMC3639690_CRIM.EM2013-198617.003.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "2Otv7awtn0kS"
   },
   "outputs": [],
   "source": [
    "generate_caption(\"PMC5603107_CRIM2017-3531823.001.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "gAlC93CPn-f5"
   },
   "outputs": [],
   "source": [
    "generate_caption(\"PMC3854574_10-1055-s-0033-1337123-i1200056-3.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "8v7BTYChoO04"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q flask flask-ngrok python-dotenv google-generativeai pillow pyngrok\n",
    "\n",
    "from flask import Flask, render_template_string, request, jsonify\n",
    "from flask_ngrok import run_with_ngrok\n",
    "import google.generativeai as genai\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "from pyngrok import ngrok\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Configure ngrok with your authtoken\n",
    "NGROK_AUTH_TOKEN = \"2sXaFpQ0oFUgu0weIUw0rXpiZXx_6ifSgEBj4JjZtW7KkvrNo\"\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "\n",
    "# Initialize ngrok tunnel\n",
    "public_url = ngrok.connect(5000).public_url\n",
    "print(\" * Public URL:\", public_url)\n",
    "run_with_ngrok(app)\n",
    "\n",
    "# Configure Gemini\n",
    "GOOGLE_API_KEY = \"AIzaSyALV-PhCw5QDYcXqKL1raV2doonlBiBrkA\"\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Set up the model\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "# HTML template with improved functionality\n",
    "HTML_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "  <meta charset=\"UTF-8\"/>\n",
    "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"/>\n",
    "  <title>AI Medical Image Captioning</title>\n",
    "  <style>\n",
    "    .home {\n",
    "      text-decoration: none;\n",
    "      background-color: #007bff;\n",
    "      color: white;\n",
    "      padding: 10px 20px;\n",
    "      border-radius: 8px;\n",
    "      font-family: Arial, sans-serif;\n",
    "      font-size: 16px;\n",
    "      position: absolute;\n",
    "      top: 20px;\n",
    "      left: 20px;\n",
    "    }\n",
    "\n",
    "    body {\n",
    "      font-family: 'Poppins', sans-serif;\n",
    "      background: linear-gradient(135deg, #0a0a0a, #363535);\n",
    "      color: rgb(20, 20, 20);\n",
    "      margin: 0;\n",
    "      padding: 0;\n",
    "      text-align: center;\n",
    "      background-image: url(\"https://static.vecteezy.com/system/resources/previews/037/246/957/large_2x/ai-generated-medical-advertisment-background-with-copy-space-free-photo.jpg\");\n",
    "      background-size: cover;\n",
    "      background-position: center;\n",
    "      min-height: 100vh;\n",
    "    }\n",
    "\n",
    "    .container {\n",
    "      background: rgba(116, 103, 103, 0.2);\n",
    "      padding: 30px;\n",
    "      border-radius: 15px;\n",
    "      max-width: 800px;\n",
    "      margin: 80px auto;\n",
    "      box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.3);\n",
    "      backdrop-filter: blur(10px);\n",
    "    }\n",
    "\n",
    "    h1 {\n",
    "      font-size: 32px;\n",
    "      margin-bottom: 5px;\n",
    "      color: #fff;\n",
    "    }\n",
    "\n",
    "    .subtitle {\n",
    "      font-size: 14px;\n",
    "      margin-bottom: 20px;\n",
    "      font-style: italic;\n",
    "      color: #f0f0f0;\n",
    "    }\n",
    "\n",
    "    .upload-box {\n",
    "      background: rgba(249, 236, 236, 0.3);\n",
    "      padding: 30px;\n",
    "      border-radius: 10px;\n",
    "      margin-bottom: 20px;\n",
    "    }\n",
    "\n",
    "    input[type=\"file\"] {\n",
    "      background: rgb(247, 237, 237);\n",
    "      padding: 15px;\n",
    "      border-radius: 5px;\n",
    "      border: none;\n",
    "      cursor: pointer;\n",
    "      display: block;\n",
    "      margin: 20px auto;\n",
    "      width: 90%;\n",
    "      max-width: 400px;\n",
    "    }\n",
    "\n",
    "    .btn {\n",
    "      background: #4CAF50;\n",
    "      color: white;\n",
    "      padding: 12px 25px;\n",
    "      border: none;\n",
    "      cursor: pointer;\n",
    "      border-radius: 5px;\n",
    "      font-size: 16px;\n",
    "      transition: 0.3s;\n",
    "      margin: 10px 5px;\n",
    "    }\n",
    "\n",
    "    .language-selector {\n",
    "      background: #007bff;\n",
    "      color: white;\n",
    "      padding: 10px 15px;\n",
    "      border: none;\n",
    "      border-radius: 5px;\n",
    "      font-size: 16px;\n",
    "      margin: 10px 0;\n",
    "    }\n",
    "\n",
    "    .btn:hover {\n",
    "      background: #45a049;\n",
    "      transform: scale(1.05);\n",
    "    }\n",
    "\n",
    "    .btn:disabled {\n",
    "      background: #cccccc;\n",
    "      cursor: not-allowed;\n",
    "    }\n",
    "\n",
    "    #status {\n",
    "      font-size: 16px;\n",
    "      margin: 15px 0;\n",
    "      color: #fff;\n",
    "      min-height: 20px;\n",
    "    }\n",
    "\n",
    "    .result {\n",
    "      display: flex;\n",
    "      flex-wrap: wrap;\n",
    "      gap: 20px;\n",
    "      justify-content: center;\n",
    "      align-items: flex-start;\n",
    "      margin-top: 20px;\n",
    "    }\n",
    "\n",
    "    #preview {\n",
    "      max-width: 100%;\n",
    "      max-height: 300px;\n",
    "      border-radius: 10px;\n",
    "      margin-top: 15px;\n",
    "      box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.3);\n",
    "      display: none;\n",
    "    }\n",
    "\n",
    "    .caption-text {\n",
    "      font-size: 16px;\n",
    "      color: white;\n",
    "      background-color: rgba(255, 255, 255, 0.1);\n",
    "      padding: 20px;\n",
    "      border-radius: 10px;\n",
    "      max-width: 400px;\n",
    "      text-align: left;\n",
    "      margin-top: 15px;\n",
    "    }\n",
    "\n",
    "    .loading {\n",
    "      border: 4px solid #f3f3f3;\n",
    "      border-top: 4px solid #3498db;\n",
    "      border-radius: 50%;\n",
    "      width: 30px;\n",
    "      height: 30px;\n",
    "      animation: spin 1s linear infinite;\n",
    "      margin: 20px auto;\n",
    "      display: none;\n",
    "    }\n",
    "\n",
    "    @keyframes spin {\n",
    "      0% { transform: rotate(0deg); }\n",
    "      100% { transform: rotate(360deg); }\n",
    "    }\n",
    "  </style>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "\n",
    "\n",
    "  <div class=\"container\">\n",
    "    <h1>ü©∫ AI Medical Image Captioning</h1>\n",
    "    <p class=\"subtitle\">Upload an image and generate accurate medical captions</p>\n",
    "\n",
    "    <div class=\"upload-box\">\n",
    "      <!-- Language Selection Dropdown -->\n",
    "      <select id=\"language\" class=\"language-selector\">\n",
    "        <option value=\"english\">English</option>\n",
    "        <option value=\"hindi\">‡§π‡§ø‡§Ç‡§¶‡•Ä (Hindi)</option>\n",
    "      </select>\n",
    "\n",
    "      <input type=\"file\" id=\"imageInput\" accept=\"image/*\">\n",
    "      <button class=\"btn\" id=\"uploadBtn\">Upload Image</button>\n",
    "      <button class=\"btn\" id=\"generateBtn\" disabled>Generate Caption</button>\n",
    "      <div class=\"loading\" id=\"loadingSpinner\"></div>\n",
    "      <div id=\"status\"></div>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"result\">\n",
    "      <img id=\"preview\"/>\n",
    "      <div class=\"caption-text\">\n",
    "        <strong>Caption:</strong> <span id=\"caption\"></span>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <script>\n",
    "    let uploadedImage = null;\n",
    "\n",
    "    document.getElementById('imageInput').addEventListener('change', function(e) {\n",
    "      const file = e.target.files[0];\n",
    "      if (file) {\n",
    "        const reader = new FileReader();\n",
    "        reader.onload = function(event) {\n",
    "          document.getElementById('preview').src = event.target.result;\n",
    "          document.getElementById('preview').style.display = 'block';\n",
    "          document.getElementById('generateBtn').disabled = false;\n",
    "          uploadedImage = file;\n",
    "          document.getElementById('status').textContent = 'Image ready for caption generation';\n",
    "        };\n",
    "        reader.readAsDataURL(file);\n",
    "      }\n",
    "    });\n",
    "\n",
    "    document.getElementById('generateBtn').addEventListener('click', async function() {\n",
    "      if (!uploadedImage) return;\n",
    "\n",
    "      const generateBtn = document.getElementById('generateBtn');\n",
    "      const loadingSpinner = document.getElementById('loadingSpinner');\n",
    "      const status = document.getElementById('status');\n",
    "      const language = document.getElementById('language').value;\n",
    "\n",
    "      generateBtn.disabled = true;\n",
    "      loadingSpinner.style.display = 'block';\n",
    "      status.textContent = 'Generating caption...';\n",
    "\n",
    "      try {\n",
    "        const formData = new FormData();\n",
    "        formData.append('file', uploadedImage);\n",
    "        formData.append('language', language);\n",
    "\n",
    "        const response = await fetch('/generate', {\n",
    "          method: 'POST',\n",
    "          body: formData\n",
    "        });\n",
    "\n",
    "        const result = await response.json();\n",
    "\n",
    "        if (result.error) {\n",
    "          status.textContent = 'Error: ' + result.error;\n",
    "        } else {\n",
    "          document.getElementById('caption').textContent = result.caption;\n",
    "          status.textContent = language === 'english'\n",
    "            ? 'Caption generated successfully!'\n",
    "            : '‡§ï‡•à‡§™‡•ç‡§∂‡§® ‡§∏‡§´‡§≤‡§§‡§æ‡§™‡•Ç‡§∞‡•ç‡§µ‡§ï ‡§â‡§§‡•ç‡§™‡§®‡•ç‡§® ‡§π‡•Å‡§Ü!';\n",
    "        }\n",
    "      } catch (error) {\n",
    "        status.textContent = 'Error: ' + error.message;\n",
    "      } finally {\n",
    "        loadingSpinner.style.display = 'none';\n",
    "        generateBtn.disabled = false;\n",
    "      }\n",
    "    });\n",
    "\n",
    "    document.getElementById('uploadBtn').addEventListener('click', function() {\n",
    "      document.getElementById('imageInput').click();\n",
    "    });\n",
    "  </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template_string(HTML_TEMPLATE)\n",
    "\n",
    "@app.route('/generate', methods=['POST'])\n",
    "def generate_caption():\n",
    "    if 'file' not in request.files:\n",
    "        return jsonify({'error': 'No file uploaded'}), 400\n",
    "\n",
    "    file = request.files['file']\n",
    "    language = request.form.get('language', 'english')\n",
    "\n",
    "    if file.filename == '':\n",
    "        return jsonify({'error': 'No file selected'}), 400\n",
    "\n",
    "    try:\n",
    "        # Process image\n",
    "        img_bytes = file.read()\n",
    "        img = Image.open(io.BytesIO(img_bytes))\n",
    "\n",
    "       # Language-specific prompts with training details\n",
    "        prompts = {\n",
    "            'english': [\n",
    "                        \" You are a medical image captioning assistant trained on the ROCO dataset, which contains radiology images and their associated clinical captions. Generate a detailed and clinically relevant caption similar in style to the ROCO dataset, such as: startseq an ap chest x ray in the trauma bay showed no obvious pneumothorax hemothorax or bullet fragment endseq Please maintain the medical terminology and tone used in ROCO-style captions\",\n",
    "\n",
    "                  ],\n",
    "            'hindi': [\n",
    "               \" You are a medical image captioning assistant trained on the ROCO dataset, which contains radiology images and their associated clinical captions. Generate a detailed and clinically relevant caption similar in style to the ROCO dataset, such as: startseq an ap chest x ray in the trauma bay showed no obvious pneumothorax hemothorax or bullet fragment endseq Please maintain the medical terminology and tone used in ROCO-style captions translate the caption generated in hindi\",\n",
    "               img\n",
    "              ]\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "        response = model.generate_content(prompts[language])\n",
    "\n",
    "        # Get the text response\n",
    "        if response.candidates and response.candidates[0].content.parts:\n",
    "            caption = response.candidates[0].content.parts[0].text\n",
    "        else:\n",
    "            caption = \"Could not generate caption for this image.\" if language == 'english' else \"‡§á‡§∏ ‡§õ‡§µ‡§ø ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡•à‡§™‡•ç‡§∂‡§® ‡§â‡§§‡•ç‡§™‡§®‡•ç‡§® ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∏‡§ï‡§æ\"\n",
    "\n",
    "        return jsonify({'caption': caption})\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e) if language == 'english' else \"‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: ‡§ï‡•à‡§™‡•ç‡§∂‡§® ‡§â‡§§‡•ç‡§™‡§®‡•ç‡§® ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§´‡§≤\"\n",
    "        return jsonify({'error': error_msg}), 500\n",
    "\n",
    "print(\"\\n\\n * IMPORTANT: Click the ngrok link below to access your app!\")\n",
    "app.run()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
